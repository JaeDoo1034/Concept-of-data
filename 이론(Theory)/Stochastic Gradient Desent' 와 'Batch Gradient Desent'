* 케글 코리아에서 나온 질문 및 답변

Q)
안녕하세요.
'Stochastic Gradient Desent' 와 'Batch Gradient Desent'에 대해 궁금한게 있습니다.

1000개의 데이터가 있고, 회귀모형을 만들 때 배치같은 경우에는 전체 데이터인 1000개를 이용해서 오차를 계산하고, 
또 오차를 줄이는 방향으로 theta를 수정하는 것으로 알고 있습니다.

그런데 스토케스틱 같은 경우에는 데이터 1개씩 따로따로 오차를 계산한다고 되어있는데 데이터 1개로 회귀식을 만들 수가 있나요? 
그리고 그렇다면 오차도 발생하지 않지 않나요? 어떤 개념인지 찾아봐도 잘 모르겠어서 질문드립니다.

A)
네 오차가 데이터마다 나오게됩니다
traditional gradient descent의 경우에는 전체 데이터 1000개를 다 살펴보고 나서야만 
전체 합산 오차(loss 정의에 따라 합산이 될수도 있고 합산의 제곱일 수도 있겠죵?)를 통해 갱신하게 되겠지만,
stochastic의 경우 데이터 1개만 보고 오차를 계산한 뒤에 바로 갱신해요!

저는 '갱신하는 시점'에 집중하면 이해하기 더 편했던 것 같아용!
