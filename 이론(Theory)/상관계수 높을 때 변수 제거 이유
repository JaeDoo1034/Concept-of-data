Q) Feature들의 correlation 값이 높을때 제거하는 이유가 뭔가요? 내용이 구체적으로 나와있는 커널은 없는거같아서요 혹시 아시는분?



A1) 
머신러닝 예측성능 관점에서는 메모리 점유 등의 이유로 컴퓨팅파워가 무의미하게 소모되는 것 외의 이슈는 없는 것으로 알고 있습니다.

feature의 의미와 기여도를 파악해야 하는 데이터 분석 관점에서는, 예컨대 X1과 X2가 완전히 동일한 feature라면 이런 식이 발생할 수 있습니다.

(1) Y = 3X1 + 2X2 + ...
(2) Y = 9X1 - 4X2 + ....

두 식을 보면 X1을 1 늘렸을 때 Y에 미치는 영향력이 3배 차이인데 실상은 똑같죠. 이런 오독을 막기 위해서라도 제거하는 것이 좋습니다.


A2)
"feature들끼리의 correlation"을 얘기하시는 거라면 유사한 feature들이 많을 수록 feature들이 가진 정보량은 그대로인데 
데이터의 차원은 계속 늘어 curse of dimensionality(차원의 저주) 문제를 야기할 수 있기 때문입니다
